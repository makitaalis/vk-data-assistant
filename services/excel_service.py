"""–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Excel —Ñ–∞–π–ª–∞–º–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∞–Ω–∞–ª–∏–∑–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""

import pandas as pd
import re
import logging
from typing import List, Dict, Tuple, Optional, Any, Set
from pathlib import Path
import json
from collections import Counter, OrderedDict
from openpyxl.styles import Alignment

from bot.config import VK_LINK_PATTERN

logger = logging.getLogger("excel_service")


class ExcelProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ Excel —Ñ–∞–π–ª–æ–≤ —Å –∞–Ω–∞–ª–∏–∑–æ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""

    def __init__(self):
        self.original_df = None
        self.vk_column_index = None
        self.vk_column_name = None
        self.vk_links_mapping = {}  # {link: [row_index, ...]}
        self.all_links_found = []  # –í—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ (—Å –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏)
        self.duplicate_analysis = None  # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤

    def find_vk_column(self, df: pd.DataFrame) -> Optional[Tuple[int, str]]:
        """
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏—Ç —Å—Ç–æ–ª–±–µ—Ü —Å–æ —Å—Å—ã–ª–∫–∞–º–∏ VK
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (–∏–Ω–¥–µ–∫—Å_—Å—Ç–æ–ª–±—Ü–∞, –∏–º—è_—Å—Ç–æ–ª–±—Ü–∞) –∏–ª–∏ None
        """
        logger.info("üîç –ü–æ–∏—Å–∫ —Å—Ç–æ–ª–±—Ü–∞ —Å–æ —Å—Å—ã–ª–∫–∞–º–∏ VK...")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–π —Å—Ç–æ–ª–±–µ—Ü
        for col_idx, col_name in enumerate(df.columns):
            vk_links_count = 0
            total_non_empty = 0

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å—Ç–æ–ª–±—Ü–µ
            for value in df[col_name].dropna():
                str_value = str(value).strip()
                if str_value:
                    total_non_empty += 1
                    if re.match(VK_LINK_PATTERN, str_value):
                        vk_links_count += 1

            # –ï—Å–ª–∏ –±–æ–ª–µ–µ 50% –Ω–µ–ø—É—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π - —ç—Ç–æ VK —Å—Å—ã–ª–∫–∏
            if total_non_empty > 0 and (vk_links_count / total_non_empty) > 0.5:
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω —Å—Ç–æ–ª–±–µ—Ü —Å–æ —Å—Å—ã–ª–∫–∞–º–∏: '{col_name}' (–∏–Ω–¥–µ–∫—Å {col_idx})")
                logger.info(f"   –°–æ–¥–µ—Ä–∂–∏—Ç {vk_links_count} VK —Å—Å—ã–ª–æ–∫ –∏–∑ {total_non_empty} –∑–Ω–∞—á–µ–Ω–∏–π")
                return col_idx, col_name

        logger.warning("‚ùå –°—Ç–æ–ª–±–µ—Ü —Å–æ —Å—Å—ã–ª–∫–∞–º–∏ VK –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return None

    def load_excel_file(self, file_path: Path) -> Tuple[List[str], List[int], bool]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç Excel —Ñ–∞–π–ª –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ VK

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        - –°–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ VK (–≤ –ø–æ—Ä—è–¥–∫–µ –ø–µ—Ä–≤–æ–≥–æ –ø–æ—è–≤–ª–µ–Ω–∏—è)
        - –°–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤ —Å—Ç—Ä–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Å—ã–ª–∫–∏
        - –£—Å–ø–µ—à–Ω–æ—Å—Ç—å –æ–ø–µ—Ä–∞—Ü–∏–∏
        """
        try:
            # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª –ë–ï–ó –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ —Å—Ç—Ä–æ–∫–∏
            self.original_df = pd.read_excel(file_path)

            # –ù–æ –¥–ª—è —Å—Ç–æ–ª–±—Ü–∞ —Å VK —Å—Å—ã–ª–∫–∞–º–∏ –Ω—É–∂–Ω–æ —É–±–µ–¥–∏—Ç—å—Å—è —á—Ç–æ –æ–Ω–∏ —Å—Ç—Ä–æ–∫–∏
            df_for_search = self.original_df.astype(str)

            logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª: {self.original_df.shape[0]} —Å—Ç—Ä–æ–∫, {self.original_df.shape[1]} —Å—Ç–æ–ª–±—Ü–æ–≤")

            # –ò—â–µ–º —Å—Ç–æ–ª–±–µ—Ü —Å–æ —Å—Å—ã–ª–∫–∞–º–∏ –≤ —Å—Ç—Ä–æ–∫–æ–≤–æ–π –≤–µ—Ä—Å–∏–∏
            column_info = self._find_vk_column_in_df(df_for_search)
            if not column_info:
                return [], [], False

            self.vk_column_index, self.vk_column_name = column_info

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –í–°–ï —Å—Å—ã–ª–∫–∏ (–≤–∫–ª—é—á–∞—è –¥—É–±–ª–∏–∫–∞—Ç—ã) –∏ –∑–∞–ø–æ–º–∏–Ω–∞–µ–º –∏—Ö –ø–æ–∑–∏—Ü–∏–∏
            self.all_links_found = []
            links_with_rows = []  # [(link, row_index), ...]
            self.vk_links_mapping = {}

            for idx, row in self.original_df.iterrows():
                # –î–ª—è VK —Å—Å—ã–ª–æ–∫ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç—Ä–æ–∫–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
                value = str(row[self.vk_column_name]).strip()

                # –ò—â–µ–º –í–°–ï VK —Å—Å—ã–ª–∫–∏ –≤ —è—á–µ–π–∫–µ (–º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ)
                matches = re.findall(VK_LINK_PATTERN, value)
                for match in matches:
                    self.all_links_found.append(match)
                    links_with_rows.append((match, idx))
                    self.vk_links_mapping.setdefault(match, []).append(idx)

            # –°–æ–∑–¥–∞–µ–º —É–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ (—Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫ –ø–µ—Ä–≤–æ–≥–æ –ø–æ—è–≤–ª–µ–Ω–∏—è)
            seen = set()
            unique_links = []
            row_indices = []
            for link, row_idx in links_with_rows:
                if link not in seen:
                    seen.add(link)
                    unique_links.append(link)
                    row_indices.append(row_idx)

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            self.duplicate_analysis = self._analyze_duplicates()

            logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(self.all_links_found)} VK —Å—Å—ã–ª–æ–∫ (–≤–∫–ª—é—á–∞—è –¥—É–±–ª–∏–∫–∞—Ç—ã)")
            logger.info(f"‚úÖ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: {len(unique_links)}")

            return unique_links, row_indices, True

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞: {e}")
            return [], [], False

    def _find_vk_column_in_df(self, df: pd.DataFrame) -> Optional[Tuple[int, str]]:
        """
        –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Ç–æ–ª–±—Ü–∞ —Å VK —Å—Å—ã–ª–∫–∞–º–∏
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–π —Å—Ç–æ–ª–±–µ—Ü
        for col_idx, col_name in enumerate(df.columns):
            vk_links_count = 0
            total_non_empty = 0

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å—Ç–æ–ª–±—Ü–µ
            for value in df[col_name].dropna():
                str_value = str(value).strip()
                if str_value and str_value != 'nan':
                    total_non_empty += 1
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ VK —Å—Å—ã–ª–∫–∞ –≤ –∑–Ω–∞—á–µ–Ω–∏–∏
                    if re.search(VK_LINK_PATTERN, str_value):
                        vk_links_count += 1

            # –ï—Å–ª–∏ –±–æ–ª–µ–µ 50% –Ω–µ–ø—É—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π —Å–æ–¥–µ—Ä–∂–∞—Ç VK —Å—Å—ã–ª–∫–∏
            if total_non_empty > 0 and (vk_links_count / total_non_empty) > 0.5:
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω —Å—Ç–æ–ª–±–µ—Ü —Å–æ —Å—Å—ã–ª–∫–∞–º–∏: '{col_name}' (–∏–Ω–¥–µ–∫—Å {col_idx})")
                logger.info(f"   –°–æ–¥–µ—Ä–∂–∏—Ç VK —Å—Å—ã–ª–∫–∏ –≤ {vk_links_count} —è—á–µ–π–∫–∞—Ö –∏–∑ {total_non_empty}")
                return col_idx, col_name

        logger.warning("‚ùå –°—Ç–æ–ª–±–µ—Ü —Å–æ —Å—Å—ã–ª–∫–∞–º–∏ VK –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return None

    def _analyze_duplicates(self) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –≤ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Å—ã–ª–∫–∞—Ö
        """
        if not self.all_links_found:
            return {
                'total_links': 0,
                'unique_links': 0,
                'duplicate_count': 0,
                'duplicate_percent': 0,
                'duplicates': {},
                'duplicate_rows': {}
            }

        # –ü–æ–¥—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç—ã –∫–∞–∂–¥–æ–π —Å—Å—ã–ª–∫–∏
        link_counter = Counter(self.all_links_found)

        # –ù–∞—Ö–æ–¥–∏–º –¥—É–±–ª–∏–∫–∞—Ç—ã (—Å—Å—ã–ª–∫–∏ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è –±–æ–ª–µ–µ 1 —Ä–∞–∑–∞)
        duplicates = {link: count for link, count in link_counter.items() if count > 1}

        # –ù–∞—Ö–æ–¥–∏–º —Å—Ç—Ä–æ–∫–∏ —Å –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏
        duplicate_rows = {}
        for idx, row in self.original_df.iterrows():
            value = str(row[self.vk_column_name]).strip()
            matches = re.findall(VK_LINK_PATTERN, value)

            for match in matches:
                if match in duplicates:
                    if match not in duplicate_rows:
                        duplicate_rows[match] = []
                    duplicate_rows[match].append(idx + 2)  # +2 –¥–ª—è Excel (1-based + –∑–∞–≥–æ–ª–æ–≤–æ–∫)

        total = len(self.all_links_found)
        unique = len(link_counter)
        duplicate_count = total - unique
        duplicate_percent = (duplicate_count / total * 100) if total > 0 else 0

        return {
            'total_links': total,
            'unique_links': unique,
            'duplicate_count': duplicate_count,
            'duplicate_percent': duplicate_percent,
            'duplicates': duplicates,
            'duplicate_rows': duplicate_rows,
            'top_duplicates': sorted(duplicates.items(), key=lambda x: x[1], reverse=True)[:10] if duplicates else []
        }

    def get_duplicate_analysis(self) -> Dict[str, Any]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        """
        if self.duplicate_analysis is None:
            self.duplicate_analysis = self._analyze_duplicates()
        return self.duplicate_analysis

    def remove_duplicates_keep_first(self) -> Tuple[List[str], List[int]]:
        """
        –£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã, –æ—Å—Ç–∞–≤–ª—è—è –ø–µ—Ä–≤–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ –∫–∞–∂–¥–æ–π —Å—Å—ã–ª–∫–∏

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        - –°–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
        - –°–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤ —Å—Ç—Ä–æ–∫
        """
        seen = set()
        unique_links = []
        row_indices = []

        for link in self.all_links_found:
            if link not in seen:
                seen.add(link)
                unique_links.append(link)
                if link in self.vk_links_mapping and self.vk_links_mapping[link]:
                    row_indices.append(self.vk_links_mapping[link][0])

        return unique_links, row_indices

    def get_links_without_duplicates(self) -> List[str]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        """
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º OrderedDict –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ—Ä—è–¥–∫–∞
        return list(OrderedDict.fromkeys(self.all_links_found))

    def save_results_with_original_data(
            self,
            results: Dict[str, Dict[str, Any]],
            output_path: Path
    ) -> bool:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –¥–æ–±–∞–≤–ª—è—è –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ç–µ–ª–µ—Ñ–æ–Ω—ã –∫ –∏—Å—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º –±–µ–∑ –∏—Ö –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∏
        """
        try:
            if self.original_df is None:
                logger.error("‚ùå –ù–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞")
                return False

            result_df = self.original_df.copy()

            target_column = "–ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ç–µ–ª–µ—Ñ–æ–Ω—ã"
            if target_column not in result_df.columns:
                result_df[target_column] = ""
            else:
                result_df[target_column] = result_df[target_column].fillna("")

            for link, data in results.items():
                rows = self.vk_links_mapping.get(link)
                if rows is None:
                    continue

                if not isinstance(rows, list):
                    rows = [rows]

                phones = self._extract_phone_list(data)
                if not phones:
                    continue
                phones = phones[:2]
                if not phones:
                    continue

                for row_idx in rows:
                    if row_idx >= len(result_df):
                        continue
                    existing_value = result_df.at[row_idx, target_column]
                    existing_items = []
                    if pd.notna(existing_value):
                        existing_items = [
                            item.strip()
                            for item in str(existing_value).replace("\n", ",").split(",")
                            if item.strip()
                        ]

                    combined = list(OrderedDict.fromkeys(existing_items + phones))[:2]
                    result_df.at[row_idx, target_column] = ", ".join(combined)

            with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
                result_df.to_excel(writer, index=False, sheet_name='–†–µ–∑—É–ª—å—Ç–∞—Ç—ã')
                worksheet = writer.sheets['–†–µ–∑—É–ª—å—Ç–∞—Ç—ã']

                header_row = 1

                for column in worksheet.columns:
                    column_letter = column[0].column_letter
                    header_value = worksheet.cell(row=header_row, column=column[0].column)

                    if header_value.value == target_column:
                        worksheet.column_dimensions[column_letter].width = 30
                        for cell in column:
                            cell.alignment = Alignment(wrap_text=True, vertical="top")
                            cell.number_format = "@"
                    else:
                        max_length = 0
                        for cell in column:
                            try:
                                if cell.value is not None:
                                    max_length = max(max_length, len(str(cell.value)))
                            except Exception:
                                continue
                        worksheet.column_dimensions[column_letter].width = min(max_length + 2, 50)

            logger.info(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_path} —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏")
            return True

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    def get_file_info(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º —Ñ–∞–π–ª–µ"""
        if self.original_df is None:
            return {}

        duplicate_info = self.get_duplicate_analysis()

        return {
            "total_rows": len(self.original_df),
            "total_columns": len(self.original_df.columns),
            "vk_column": self.vk_column_name,
            "vk_links_count": len(self.vk_links_mapping),
            "total_links_found": duplicate_info['total_links'],
            "duplicate_count": duplicate_info['duplicate_count'],
            "duplicate_percent": duplicate_info['duplicate_percent'],
            "columns": list(self.original_df.columns)
        }

    def _extract_phone_list(self, result_data: Dict[str, Any]) -> List[str]:
        """–í—ã—Ç–∞—Å–∫–∏–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∏ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Å—Ç—Ä–æ–∫–æ–≤–æ–º—É –≤–∏–¥—É"""
        phones = result_data.get("phones", [])

        if phones is None:
            phones = []
        elif isinstance(phones, str):
            if phones.startswith('['):
                try:
                    phones = json.loads(phones)
                except Exception:
                    phones = []
            else:
                phones = [phones] if phones else []
        elif not isinstance(phones, list):
            phones = []

        normalized = OrderedDict()
        for phone in phones:
            candidate = str(phone).strip()
            if candidate:
                normalized[candidate] = None

        return list(normalized.keys())

    def update_results_from_dict(self, results: Dict[str, Dict[str, Any]]):
        """
        –û–±–Ω–æ–≤–ª—è–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–π DataFrame —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–æ–∏—Å–∫–∞
        –î–æ–±–∞–≤–ª—è–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤, –∏–º–µ–Ω –∏ –¥–∞—Ç —Ä–æ–∂–¥–µ–Ω–∏—è
        """
        if self.original_df is None:
            logger.error("‚ùå –ù–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ DataFrame –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è")
            return

        try:
            # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–ª—è —Ä–∞–±–æ—Ç—ã
            df = self.original_df.copy()
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫
            max_phones = max(
                len(data.get('phones', [])) 
                for data in results.values() 
                if isinstance(data.get('phones'), list)
            ) if results else 0
            max_phones = min(max_phones, 2)
            
            # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            phone_columns = []
            for i in range(max_phones):
                col_name = f"Phone_{i+1}" if i > 0 else "Phone"
                if col_name not in df.columns:
                    df[col_name] = ""
                phone_columns.append(col_name)
            
            # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –¥—Ä—É–≥–∏—Ö –¥–∞–Ω–Ω—ã—Ö –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if "Full_Name" not in df.columns:
                df["Full_Name"] = ""
            if "Birth_Date" not in df.columns:
                df["Birth_Date"] = ""
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Å—ã–ª–∫–∏
            updated_count = 0
            for link, data in results.items():
                if link not in self.vk_links_mapping:
                    continue

                row_indices = self.vk_links_mapping.get(link) or []
                if not isinstance(row_indices, list):
                    row_indices = [row_indices]

                # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–ª–µ—Ñ–æ–Ω—ã
                phones = data.get('phones', [])
                phone_values = phones if isinstance(phones, list) else []
                phone_values = phone_values[:2]

                # –û–±–Ω–æ–≤–ª—è–µ–º –∏–º—è/–¥–∞—Ç—É
                full_name = data.get('full_name', '').strip()
                birth_date = data.get('birth_date', '').strip()

                for row_idx in row_indices:
                    for i, phone in enumerate(phone_values[:max_phones]):
                        if i < len(phone_columns):
                            df.at[row_idx, phone_columns[i]] = phone

                    if full_name:
                        df.at[row_idx, "Full_Name"] = full_name
                    if birth_date:
                        df.at[row_idx, "Birth_Date"] = birth_date

                updated_count += 1
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π DataFrame
            self.original_df = df
            logger.info(f"‚úÖ DataFrame –æ–±–Ω–æ–≤–ª–µ–Ω —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –¥–ª—è {updated_count} —Å—Å—ã–ª–æ–∫")
            logger.info(f"   –î–æ–±–∞–≤–ª–µ–Ω–æ –∫–æ–ª–æ–Ω–æ–∫: Phone({max_phones}), Full_Name, Birth_Date")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ DataFrame: {e}")
            import traceback
            logger.error(traceback.format_exc())
