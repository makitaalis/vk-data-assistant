import pandas as pd
import re
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple, Set
import json
import asyncio

logger = logging.getLogger("db_loader")

# –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Å—ã–ª–æ–∫ VK
VK_LINK_PATTERN = r'https?://(?:www\.)?(?:vk\.com|m\.vk\.com)/(?:id\d+|[a-zA-Z0-9_\.]+)'

# –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤
PHONE_PATTERN = r'(?<!\d)(?:7|8|9)\d{10}(?!\d)'


class DatabaseLoader:
    """–ö–ª–∞—Å—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Excel —Ñ–∞–π–ª–æ–≤ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""

    def __init__(self, database):
        self.db = database

    def normalize_phone(self, phone: str) -> Optional[str]:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã–π –Ω–æ–º–µ—Ä –∫ –µ–¥–∏–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É"""
        # –û—á–∏—â–∞–µ–º –æ—Ç –≤—Å–µ—Ö –Ω–µ—Ü–∏—Ñ—Ä–æ–≤—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        digits = re.sub(r'[^\d]', '', phone)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
        if len(digits) == 11 and digits.startswith('7'):
            return digits
        elif len(digits) == 11 and digits.startswith('8'):
            return '7' + digits[1:]
        elif len(digits) == 10 and digits.startswith('9'):
            return '7' + digits

        return None

    def extract_data_from_row(self, row: pd.Series) -> Dict[str, Any]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ VK —Å—Å—ã–ª–∫–∏ –∏ —Ç–µ–ª–µ—Ñ–æ–Ω—ã –∏–∑ —Å—Ç—Ä–æ–∫–∏, –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã

        Returns:
            Dict —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏: {"vk_links": [...], "phones": [...]}
        """
        result = {
            "vk_links": [],
            "phones": [],
            "full_name": "",
            "birth_date": ""
        }

        seen_links = set()
        seen_phones = set()

        # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º —è—á–µ–π–∫–∞–º –≤ —Å—Ç—Ä–æ–∫–µ
        for value in row:
            if pd.isna(value):
                continue

            str_value = str(value).strip()
            if not str_value:
                continue

            # –ò—â–µ–º VK —Å—Å—ã–ª–∫–∏
            vk_matches = re.findall(VK_LINK_PATTERN, str_value)
            for link in vk_matches:
                if link not in seen_links:
                    result["vk_links"].append(link)
                    seen_links.add(link)

            # –ò—â–µ–º —Ç–µ–ª–µ—Ñ–æ–Ω—ã
            phone_matches = re.findall(PHONE_PATTERN, str_value)
            for phone in phone_matches:
                normalized = self.normalize_phone(phone)
                if normalized and normalized not in seen_phones:
                    result["phones"].append(normalized)
                    seen_phones.add(normalized)

            # –ü—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∏–º—è (—Å—Ç—Ä–æ–∫–∞ –±–µ–∑ —Å—Å—ã–ª–æ–∫ –∏ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤, –¥–ª–∏–Ω–æ–π –æ—Ç 5 –¥–æ 50 —Å–∏–º–≤–æ–ª–æ–≤)
            if not result["full_name"] and 5 <= len(str_value) <= 50:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ —Å—Å—ã–ª–∫–∞ –∏ –Ω–µ —Ç–µ–ª–µ—Ñ–æ–Ω
                if not re.search(VK_LINK_PATTERN, str_value) and not re.search(PHONE_PATTERN, str_value):
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ –∏–º—è (—Å–æ–¥–µ—Ä–∂–∏—Ç –±—É–∫–≤—ã)
                    if re.search(r'[–∞-—è–ê-–Øa-zA-Z]', str_value):
                        result["full_name"] = str_value

            # –ü—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –¥–∞—Ç—É —Ä–æ–∂–¥–µ–Ω–∏—è
            if not result["birth_date"]:
                # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –¥–∞—Ç
                date_patterns = [
                    r'\d{1,2}\.\d{1,2}\.\d{4}',  # 12.08.2003
                    r'\d{1,2}\.\d{1,2}\.\d{2}',  # 12.08.03
                    r'\d{4}-\d{2}-\d{2}',  # 2003-08-12
                ]

                for pattern in date_patterns:
                    date_match = re.search(pattern, str_value)
                    if date_match:
                        result["birth_date"] = date_match.group()
                        break

        return result

    async def check_duplicates_in_batch(self, records: List[Dict[str, Any]]) -> Tuple[
        List[Dict[str, Any]], Dict[str, Any]]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤—Å–µ –∑–∞–ø–∏—Å–∏ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ

        Returns:
            Tuple[List[Dict], Dict]: (—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–∞–ø–∏—Å–∏, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤)
        """
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –∏ —Ç–µ–ª–µ—Ñ–æ–Ω—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        all_links = []
        all_phones = []

        for record in records:
            if record["link"] and not record["link"].startswith("phone:"):
                all_links.append(record["link"])
            all_phones.extend(record.get("phones", []))

        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –≤ —Å–ø–∏—Å–∫–∞—Ö
        unique_links = list(set(all_links))
        unique_phones = list(set(all_phones))

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –≤ –ë–î
        duplicate_data = await self.db.check_both_duplicates(unique_links, unique_phones)
        duplicate_links = duplicate_data["duplicate_links"]
        duplicate_phones = duplicate_data["duplicate_phones"]

        # –§–∏–ª—å—Ç—Ä—É–µ–º –∑–∞–ø–∏—Å–∏
        unique_records = []
        duplicate_stats = {
            "total_checked": len(records),
            "duplicate_by_link": 0,
            "duplicate_by_phone": 0,
            "duplicate_by_both": 0,
            "unique": 0
        }

        for record in records:
            link = record["link"]
            phones = record.get("phones", [])

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            is_duplicate_link = link in duplicate_links
            is_duplicate_phone = any(phone in duplicate_phones for phone in phones)

            if is_duplicate_link and is_duplicate_phone:
                duplicate_stats["duplicate_by_both"] += 1
                logger.info(f"üîÑ –î—É–±–ª–∏–∫–∞—Ç –ø–æ —Å—Å—ã–ª–∫–µ –ò —Ç–µ–ª–µ—Ñ–æ–Ω—É: {link}")
            elif is_duplicate_link:
                duplicate_stats["duplicate_by_link"] += 1
                logger.info(f"üîÑ –î—É–±–ª–∏–∫–∞—Ç –ø–æ —Å—Å—ã–ª–∫–µ: {link}")
            elif is_duplicate_phone:
                duplicate_stats["duplicate_by_phone"] += 1
                logger.info(f"üîÑ –î—É–±–ª–∏–∫–∞—Ç –ø–æ —Ç–µ–ª–µ—Ñ–æ–Ω—É: {link} - —Ç–µ–ª–µ—Ñ–æ–Ω—ã {phones}")
            else:
                # –≠—Ç–æ —É–Ω–∏–∫–∞–ª—å–Ω–∞—è –∑–∞–ø–∏—Å—å
                unique_records.append(record)
                duplicate_stats["unique"] += 1

        return unique_records, duplicate_stats

    def process_excel_file(self, file_path: Path) -> Tuple[List[Dict[str, Any]], Dict[str, int]]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç Excel —Ñ–∞–π–ª –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ

        Returns:
            Tuple[List[Dict], Dict]: (—Å–ø–∏—Å–æ–∫ –∑–∞–ø–∏—Å–µ–π, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞)
        """
        stats = {
            "total_rows": 0,
            "rows_with_vk_links": 0,
            "rows_with_phones": 0,
            "total_vk_links": 0,
            "total_phones": 0,
            "unique_vk_links": 0,
            "unique_phones": 0
        }

        all_records = []
        vk_link_to_data = {}  # {vk_link: {"phones": [], "full_name": "", "birth_date": ""}}
        phone_to_vk_links = {}  # {phone: [vk_links]}

        try:
            # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª, –ø—Ä–æ–±—É–µ–º —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –∏ –±–µ–∑
            try:
                df = pd.read_excel(file_path, dtype=str)
            except:
                df = pd.read_excel(file_path, dtype=str, header=None)

            stats["total_rows"] = len(df)
            logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª: {stats['total_rows']} —Å—Ç—Ä–æ–∫, {len(df.columns)} —Å—Ç–æ–ª–±—Ü–æ–≤")

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é —Å—Ç—Ä–æ–∫—É
            for idx, row in df.iterrows():
                try:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Å—Ç—Ä–æ–∫–∏
                    row_data = self.extract_data_from_row(row)

                    if row_data["vk_links"]:
                        stats["rows_with_vk_links"] += 1
                        stats["total_vk_links"] += len(row_data["vk_links"])

                    if row_data["phones"]:
                        stats["rows_with_phones"] += 1
                        stats["total_phones"] += len(row_data["phones"])

                    # –°–≤—è–∑—ã–≤–∞–µ–º VK —Å—Å—ã–ª–∫–∏ —Å –¥–∞–Ω–Ω—ã–º–∏
                    for vk_link in row_data["vk_links"]:
                        if vk_link not in vk_link_to_data:
                            vk_link_to_data[vk_link] = {
                                "phones": [],
                                "full_name": row_data["full_name"],
                                "birth_date": row_data["birth_date"]
                            }

                        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–ª–µ—Ñ–æ–Ω—ã –∫ —Å—Å—ã–ª–∫–µ
                        for phone in row_data["phones"]:
                            if phone not in vk_link_to_data[vk_link]["phones"]:
                                vk_link_to_data[vk_link]["phones"].append(phone)

                        # –û–±–Ω–æ–≤–ª—è–µ–º –∏–º—è –∏ –¥–∞—Ç—É –µ—Å–ª–∏ –æ–Ω–∏ –ø—É—Å—Ç—ã–µ
                        if not vk_link_to_data[vk_link]["full_name"] and row_data["full_name"]:
                            vk_link_to_data[vk_link]["full_name"] = row_data["full_name"]

                        if not vk_link_to_data[vk_link]["birth_date"] and row_data["birth_date"]:
                            vk_link_to_data[vk_link]["birth_date"] = row_data["birth_date"]

                    # –°–≤—è–∑—ã–≤–∞–µ–º —Ç–µ–ª–µ—Ñ–æ–Ω—ã —Å VK —Å—Å—ã–ª–∫–∞–º–∏
                    for phone in row_data["phones"]:
                        if phone not in phone_to_vk_links:
                            phone_to_vk_links[phone] = []

                        for vk_link in row_data["vk_links"]:
                            if vk_link not in phone_to_vk_links[phone]:
                                phone_to_vk_links[phone].append(vk_link)

                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–æ–∫–∏ {idx}: {e}")
                    continue

            # –¢–µ–ø–µ—Ä—å –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–ª–µ—Ñ–æ–Ω—ã –±–µ–∑ VK —Å—Å—ã–ª–æ–∫
            # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å–∏ –¥–ª—è —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤, —É –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –ø—Ä—è–º–æ–π —Å–≤—è–∑–∏ —Å VK
            orphan_phones = {}  # –¢–µ–ª–µ—Ñ–æ–Ω—ã –±–µ–∑ –ø—Ä—è–º—ã—Ö VK —Å—Å—ã–ª–æ–∫

            for idx, row in df.iterrows():
                row_data = self.extract_data_from_row(row)

                # –ï—Å–ª–∏ –µ—Å—Ç—å —Ç–µ–ª–µ—Ñ–æ–Ω—ã, –Ω–æ –Ω–µ—Ç VK —Å—Å—ã–ª–æ–∫
                if row_data["phones"] and not row_data["vk_links"]:
                    for phone in row_data["phones"]:
                        if phone not in orphan_phones:
                            orphan_phones[phone] = {
                                "full_name": row_data["full_name"],
                                "birth_date": row_data["birth_date"]
                            }

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –∑–∞–ø–∏—Å–∏
            # 1. –ó–∞–ø–∏—Å–∏ —Å VK —Å—Å—ã–ª–∫–∞–º–∏
            for vk_link, data in vk_link_to_data.items():
                record = {
                    "link": vk_link,
                    "phones": data["phones"],
                    "full_name": data["full_name"],
                    "birth_date": data["birth_date"]
                }
                all_records.append(record)

            # 2. –ó–∞–ø–∏—Å–∏ –¥–ª—è —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ –±–µ–∑ VK —Å—Å—ã–ª–æ–∫ (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∑–∞–ø–∏—Å–∏)
            for phone, data in orphan_phones.items():
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ —ç—Ç–æ–≥–æ —Ç–µ–ª–µ—Ñ–æ–Ω–∞ —É–∂–µ –≤ –±–∞–∑–µ
                # –≠—Ç–æ –±—É–¥–µ—Ç –¥–µ–ª–∞—Ç—å –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–¥ —á–µ—Ä–µ–∑ check_phone_duplicates
                record = {
                    "link": f"phone:{phone}",  # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ –±–µ–∑ —Å—Å—ã–ª–æ–∫
                    "phones": [phone],
                    "full_name": data["full_name"],
                    "birth_date": data["birth_date"]
                }
                all_records.append(record)

            stats["unique_vk_links"] = len(vk_link_to_data)
            stats["unique_phones"] = len(phone_to_vk_links)

            logger.info(f"üìä –ò—Ç–æ–≥–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
            logger.info(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö VK —Å—Å—ã–ª–æ–∫: {stats['unique_vk_links']}")
            logger.info(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤: {stats['unique_phones']}")
            logger.info(f"   –¢–µ–ª–µ—Ñ–æ–Ω–æ–≤ –±–µ–∑ VK —Å—Å—ã–ª–æ–∫: {len(orphan_phones)}")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
            import traceback
            logger.error(traceback.format_exc())

        return all_records, stats

    async def load_from_excel(self, file_path: Path, user_id: int) -> Dict[str, int]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ Excel —Ñ–∞–π–ª–∞ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö

        Args:
            file_path: –ü—É—Ç—å –∫ Excel —Ñ–∞–π–ª—É
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –∑–∞–≥—Ä—É–∂–∞—é—â–µ–≥–æ –¥–∞–Ω–Ω—ã–µ

        Returns:
            Dict —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
        """
        logger.info(f"üîÑ –ù–∞—á–∏–Ω–∞—é –∑–∞–≥—Ä—É–∑–∫—É —Ñ–∞–π–ª–∞: {file_path.name}")

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª
        records, file_stats = self.process_excel_file(file_path)

        logger.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(records)} –∑–∞–ø–∏—Å–µ–π")

        if not records:
            return {"added": 0, "updated": 0, "errors": 0, "duplicates": 0}

        # –§–∏–ª—å—Ç—Ä—É–µ–º –∑–∞–ø–∏—Å–∏ —Å –Ω–∞—Å—Ç–æ—è—â–∏–º–∏ VK —Å—Å—ã–ª–∫–∞–º–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        vk_records = [r for r in records if not r["link"].startswith("phone:")]

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
        unique_records, duplicate_stats = await self.check_duplicates_in_batch(vk_records)

        logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤:")
        logger.info(f"   –í—Å–µ–≥–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ: {duplicate_stats['total_checked']}")
        logger.info(f"   –î—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Å—Å—ã–ª–∫–µ: {duplicate_stats['duplicate_by_link']}")
        logger.info(f"   –î—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Ç–µ–ª–µ—Ñ–æ–Ω—É: {duplicate_stats['duplicate_by_phone']}")
        logger.info(f"   –î—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –æ–±–æ–∏–º: {duplicate_stats['duplicate_by_both']}")
        logger.info(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: {duplicate_stats['unique']}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö —Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–∞–ø–∏—Å–∏
        if unique_records:
            db_stats = await self.db.batch_save_results(unique_records, user_id, source="import")
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –≤ –ë–î: –¥–æ–±–∞–≤–ª–µ–Ω–æ {db_stats['added']}, –æ–±–Ω–æ–≤–ª–µ–Ω–æ {db_stats['updated']}")
        else:
            db_stats = {"added": 0, "updated": 0, "errors": 0}
            logger.info("‚ö†Ô∏è –ù–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥—É–±–ª–∏–∫–∞—Ç–∞—Ö –≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        db_stats["duplicates"] = duplicate_stats['total_checked'] - duplicate_stats['unique']

        return db_stats

    def analyze_excel_structure(self, file_path: Path) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É Excel —Ñ–∞–π–ª–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏"""
        try:
            # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
            try:
                df = pd.read_excel(file_path, dtype=str)
            except:
                df = pd.read_excel(file_path, dtype=str, header=None)

            analysis = {
                "file_name": file_path.name,
                "total_rows": len(df),
                "total_columns": len(df.columns),
                "data_preview": []
            }

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫
            for idx, row in df.head().iterrows():
                row_data = self.extract_data_from_row(row)
                analysis["data_preview"].append({
                    "row": idx + 1,
                    "vk_links": row_data["vk_links"],
                    "phones": row_data["phones"],
                    "full_name": row_data["full_name"],
                    "birth_date": row_data["birth_date"]
                })

            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            all_vk_links = set()
            all_phones = set()

            for idx, row in df.iterrows():
                row_data = self.extract_data_from_row(row)
                all_vk_links.update(row_data["vk_links"])
                all_phones.update(row_data["phones"])

            analysis["total_unique_vk_links"] = len(all_vk_links)
            analysis["total_unique_phones"] = len(all_phones)

            return analysis

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ñ–∞–π–ª–∞: {e}")
            return {"error": str(e)}

    def find_all_related_data(self, records: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        –ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ —Ç–µ–ª–µ—Ñ–æ–Ω–∞–º –∏ —Å—Å—ã–ª–∫–∞–º

        Returns:
            Dict —Å –ø–æ–ª–Ω–æ–π –∫–∞—Ä—Ç–∏–Ω–æ–π —Å–≤—è–∑–µ–π
        """
        phone_network = {}  # {phone: {"vk_links": [...], "names": [...], "birth_dates": [...]}}
        vk_network = {}  # {vk_link: {"phones": [...], "related_vk_links": [...]}}

        # –°—Ç—Ä–æ–∏–º —Å–µ—Ç—å —Å–≤—è–∑–µ–π
        for record in records:
            vk_link = record["link"]
            phones = record["phones"]

            # –û–±–Ω–æ–≤–ª—è–µ–º VK —Å–µ—Ç—å
            if vk_link not in vk_network:
                vk_network[vk_link] = {"phones": [], "related_vk_links": set()}
            vk_network[vk_link]["phones"] = phones

            # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—É—é —Å–µ—Ç—å
            for phone in phones:
                if phone not in phone_network:
                    phone_network[phone] = {"vk_links": [], "names": [], "birth_dates": []}

                if vk_link not in phone_network[phone]["vk_links"]:
                    phone_network[phone]["vk_links"].append(vk_link)

                if record["full_name"] and record["full_name"] not in phone_network[phone]["names"]:
                    phone_network[phone]["names"].append(record["full_name"])

                if record["birth_date"] and record["birth_date"] not in phone_network[phone]["birth_dates"]:
                    phone_network[phone]["birth_dates"].append(record["birth_date"])

        # –ù–∞—Ö–æ–¥–∏–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ VK —á–µ—Ä–µ–∑ –æ–±—â–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω—ã
        for phone, data in phone_network.items():
            vk_links = data["vk_links"]
            # –°–≤—è–∑—ã–≤–∞–µ–º –≤—Å–µ VK —Å—Å—ã–ª–∫–∏, –∏–º–µ—é—â–∏–µ –æ–±—â–∏–π —Ç–µ–ª–µ—Ñ–æ–Ω
            for i, vk1 in enumerate(vk_links):
                for vk2 in vk_links[i + 1:]:
                    vk_network[vk1]["related_vk_links"].add(vk2)
                    vk_network[vk2]["related_vk_links"].add(vk1)

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º sets –≤ lists –¥–ª—è JSON
        for vk_link in vk_network:
            vk_network[vk_link]["related_vk_links"] = list(vk_network[vk_link]["related_vk_links"])

        return {
            "phone_network": phone_network,
            "vk_network": vk_network,
            "stats": {
                "total_phones": len(phone_network),
                "phones_with_multiple_vk": sum(1 for p in phone_network.values() if len(p["vk_links"]) > 1),
                "total_vk_links": len(vk_network),
                "vk_with_multiple_phones": sum(1 for v in vk_network.values() if len(v["phones"]) > 1)
            }
        }